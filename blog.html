<html lang="en"><head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">


    <title>What are they talking about?</title>

    <!-- Bootstrap core CSS -->
    <link href="bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="bootstrap/css/blog.css" rel="stylesheet">
    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
 	<style>
	iframe {
		width:600px;
		height: 600px;
		}
	#borde { border: 2px black solid; }	
	</style>
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
	
	 </head>

   <div class="navbar-wrapper">
      <div class="container">

        <nav class="navbar navbar-inverse navbar-static-top">
          <div class="container">
            <div class="navbar-header">
              <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="true" aria-controls="navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
              <a class="navbar-brand">What are they talking about?</a>
            </div>
			<div id="navbar" class="navbar-collapse collapse">
              <ul class="nav navbar-nav">
				<li><a href="./index.html">Home</a></li>
				<li><a href="./the_radar.html">The Radar</a></li>
                <li><a href="./the_analysis.html">The Analysis</a></li>
				<li><a href="./topic_evolution.html">Topic Evolution</a></li>
				<li><a href="./politilines.html">Politilines</a></li>
				<li><a href="./words_they_used.html">Words They Used</a></li>
				<li class="active"><a>Blog</a></li>
              </ul>
            </div>
          </div>
        </nav>

      </div>
    </div>

  <body>

    <div class="container">

      <div class="blog-header">
        <h1 class="blog-title">What are they talking about?</h1>
        <p class="lead blog-description">The process: How we have done it?</p>
      </div>

      <div class="row">

        <div class="col-sm-8 blog-main">

          <div class="blog-post">
            <h2 class="blog-post-title">How does the CIS's Barometer work?</h2>
			<p> <br>  Every month 2.500 legal age people are selected randomly and proportional to other variables such as age, sex, population and environment ( <a href="http://www.cis.es/cis/export/sites/default/-Archivos/NotasdeInvestigacion/NI004_MetodologiaBarometros_Informe.pdf"> more information about the method </a>).
				These people are interviewed personally at home by CIS agents. The result of the surveys is what the barometers show us.
				<br><br>   In these barometers we see two blocks of questions. The first one is always the same and the other can change and ask about a recent political or social issue. It is from the first block where we have selected two fixed questions for our study:</p>
			 <blockquote>
              <li> ¿Cuál es, a su juicio, el principal problema que existe actualmente en España?¿Y el segundo?¿Y el tercero?(RESPUESTA ESPONTÁNEA) -Available since mar'06</li>
			  <img src="images/pregunta1_cis.png" alt="Pregunta CIS" width="700" height="250">
			  <li> ¿Cuáles son, a su juicio, los tres problemas principales que existen actualmente en España?(MÁXIMO TRES RESPUESTAS) -Available from sep'00 to feb'06</li></p>
			  <img src="images/pregunta2_cis.png" alt="Pregunta CIS" width="700" height="400">	
 		   </blockquote>
			<p> The answers to these two questions are the three main problems that we have in Spain in the opinion of the Spanish Society and this is what we are going to use for our study.
			<br><br> We can find all the barometers in the <a href="http://www.cis.es/cis/opencm/EN/11_barometros/depositados.jsp">Data Bank</a>.</p>
		</div><!-- /.blog-post -->

		<div class="blog-post">
            <h2 class="blog-post-title">How do the Control Sessions to the Government in the Congress of Deputies work?</h2>
			<p><br>There are many kinds of session done in the Congress, however we selected the Control Sessions that are taking place every Wednesday. Although they are not defined by the Constitution, they have become very popular over the years and now are ones of the most popular. Is in these sessions where the different political groups can ask to the Government and Ministers about the new laws that are in process or recently approved or talk about detected problems and they answer.
				<br><br> The session diaries are completely available in their web page by a <a href="http://www.congreso.es/portal/page/portal/Congreso/Congreso/Publicaciones">search engine</a>.
				<br><br> You can find more information about how they work in the following <a href="http://www.periodistasparlamentarios.org/?p=834">link</a>:</p>		
			<blockquote>
			<p>"[...] A normal session has a duration of at least 2 hours to debate about the proposed questions and answers. The Control Sessions are taking place on Tuesday afternoon in the Senate and Wednesday morning on the Congress.
			<br><br> In first place they debate the questions and then the speech. The Government answer to the last ones can generate an iniciative from the asking group who put together all the conclusions and became a proposal(moción consecuencia de interpelación). These ones are going to be debated in the next session [...]"</p>
			</blockquote>
		</div>		
		
		<div class="blog-post">
            <h2 class="blog-post-title">The Scrapping</h2>
			<p><br>  The Scrapping is the process from which we extract the information from the web. In this case we have achieve CIS barometers using its data bank and the Congress diaries using the Search Engine. 
				<br><br> For this part of the process we needed Selenium and BeautifulSoup. </p>
			<h3> <br> CIS </h3>
			<p> Using Selenium we have download the content of all the barometers and with the help of BeautifulSoup we have searched the question and create a database with the results. 
			<br>As answer we have 30 different topics each month, that means that during all the period (from sept'00 to feb'15) we have found 150 different topics. It is impossible to visualize all those topics and lots of them were similar. Because of this we create 14 clusters: Education, Health, Corruption, Environment, Public Services, Ideology, Economy, Employment, Youth, Justice, Social, Terrorism, Housing, Public Safety and Others. 
			<br><br> Once we have the clusters we will put into groups the data and obtain the information that is need for the charts. </p>
			<h3><br> The Congress of Deputies</h3>
			<p><br> We are interested in the data between September 2000 to March 2015, because is on this period when we have data from CIS.
			<br><br> The Congress webpage is generated dynamically. To obtain the list and download the session diaries we have used tools from Selenium and BeautifulSoap libraries. 
			<br><br> As soon as we have the documents we start the analysis of the structure to extract the information that we really need. The Corpus has a defined patter with speeches next to questions previously registered (every question has a number to identify it). Every speech is registered with the deputy who started first. Knowing this, the documents are going to be divided in questions and answers which are done in the Control Session, the deputy and the political group it pertains. 
			<br><br> To identify all the Congress members in the speeches and the political group they belong to we can obtain the information from the Congress web:<a href="http://www.congreso.es/portal/page/portal/Congreso/Congreso/Diputados?_piref73_1333056_73_1333049_1333049.next_page=/wc/menuAbecedarioInicio&tipoBusqueda=completo&idLegislatura=10">Deputies</a>.</p>
			<img src="images/congres2.png" alt="Lista Diputados" width="700" height="700">
			<p><br> We are going to use Selenium and BeautifulSoap again to a deputies relation, parliamentary groups and legislature from the analysed period. Again, all this information we are going to keep it in MongoDB to use it in the next step. </p>
			<h4> <br>Questions and speeches extraction </h4>
			<p><br> For the extraction of questions and speeches we are going to use regular expressions. With their help we can analyse line by line the content of the diary searching the start and end from the questions and the speeches, deputies, parliamentary groups, ....
			<br><br> The process starts when it finds the marks that indicate the question start, omitting the diary sections that doesn't correspond to the control sessions questions, as for example: "PROPOSICIONES NO DE LEY", "CONVALIDACIÓN O DEROGACIÓN DE REALES DECRETOS-LEYES", "JURAMENTO O PROMESA DE ACATAMIENTO DE LA CONSTITUCIÓN", etc. Also we omit in the process the interventions realised by the Congress moderator.
			<br><br> Behind this lines we show you an example of the text before and after the process(with a different background depending on the political group to whom the deputy who realises belongs: blue, grupo popular; red, grupo socialista; grey, grupo mixto,...)</p>
			<img src="images/intervencion1_eng.png" alt="Intervención Congreso" width="900" height="600">
			<p> Then we apply the <a href="http://en.wikipedia.org/wiki/String_metric">String Metrics</a> method. With this method the possible transcriptions mistakes will not affect to our analysis.
			<br><br> Finally, you can see a complet diary session after all the process. If you put the pointer on top it gives to us information about the deputy, political group and other information obtained during the analysis.</p>
			<p><iframe src="politilines/data/marked/20150311_doc_marked.html" frameborder="0" id="borde"></iframe></p>
		</div>
		
		<div class="blog-post">
            <h2 class="blog-post-title">How we find the keywords of every speech?</h2>
			<h3>Feature extraction and text clustering</h3>
			<h4><br> The problem </h4>
			<p><br>After scraping the Congreso's and the CIS’ website, our next goal was to figure out how to connect these two worlds. The problem was, we want to build a bridge between the main topics coming from the Congreso’s documents and the main topics from the CIS website. 	
			<br><br>There are a lot of ways to tackle this problem. We tried some of them, but, eventually, we chose to extract top topics from Congreso’s documents and to build a model using the k-mean algorithm in order to clustering these documents using the CIS’ main topics as labels to these clusters (as many as we can!). Thus, we could create, somehow, a dictionary between Congreso and CIS main topics. Next step was, to establish how to work out the metrics in order to compare these two worlds.
			<br><br>In the following paragraphs, we try to explain how we dealt with this problem step by step.</p>
			<h4><br>Defining what it is a "feature"</h4>
			<p><br>Since we were interested in the extraction of the main topics from our Corpus, we defined as a feature any n-gram keyword (a sequence of one or more words) within the Corpus.
			<br><br>We decided that it would be helpful for our purpose, to split the Corpus into smaller pieces to reduce the number of topics per document, where one piece consist of a single "question" and their "interventions". This is what we call a "document" in the following paragraphs.
			<br><br>We noticed that concepts, in general, are expressed by nouns, with or without adjectives, so, we focused on finding the way to extract nouns and adjectives from our documents.</p>
			<h4><br>How we extracted these features from the Corpus</h4>
			<p><br>We used an unsupervised algorithm called RAKE (Rapid Automatic Keyword Extraction) implemented in Python to do that. RAKE it is very simple and has only a parameter. This parameter is a stop list and RAKE uses it to generate the co-occurrence graph formed by the candidates keywords.
			<br><br>At this point, we considered building a customized stop list instead of using a static stop list. At first, we dealt with this problem as it was a NLP problem (Natural Language Processing), so we used a Spanish POS (Part-of-Speech) tagger to identify nouns and adjectives from our documents. So, we built stop lists as a list of all words except those identified as nouns and adjectives.
			<br><br>RAKE is also able to score the selected keywords. It works out the weight of each keyword as: deg(kw) / freq(kw)</p>
			<h4><br>Cleaning and filtering keywords</h4>
			<p><br>Before extracting keywords from each document, we used a dictionary to find false positives single out by RAKE (just in case the POS tagger have failed).
			<br><br>In the course of extracting and loading the keywords into the database (by the way, we have used MongoDB), we discarded keywords with a RAKE score below 1.0
			<br><br>After the process of extracting and loading keywords, we discarded manually those keywords that were meaningless under our point of view.</p>
			<h4><br>Unsupervised approach: Clustering documents with k-Means</h4> 
			<p><br>Before applying the k-means algorithm, we built the term-frequency matrix using the extracted keywords as a vocabulary (bag-of-the-words).
			<br><br>At the beginning we used the sckit-learn Python library to build the term-frequency matrix but we noticed that scikit-learn counted the words within a keyword as unigrams and this was not what we wanted to do, so we decided to count keywords using an own algorithm.
			<br><br>After that, we used scikit-learn library to transform this matrix into a sparse matrix (where the coefficients are calculated as “tfidf” weights). 
			<br><br>Then, we applied the k-means algorithm using the "k-means++" initialization and executing it up to 10 times to deal with variance due to the random initialization of centroids. Initially, we chose k=20 in our first tries, where k is the number of fixed clusters.</p>
			<h4><br>Tuning parameters selection: Grid search method and Silhouette coefficients</h4>
			<p><br>After these first tries, we did not have no idea about the "k" value, so we tried some values in combination with different term-frequency thresholds. So, we dealt with two parameters: term-frequency and the number of clusters.	
			<br><br>In order to find the best combination of these two values, we built a grid search algorithm to execute multiple combinations, in order to evaluate the best of them, we worked out the Silhouette coefficients for each combination and it turned out that the outcomes were very close to zero, so the points were very close each other.
			<br><br>Finally, we took k = 45 and tf = 25, where tf is the term-frequency threshold.</p>
			<h4><br>Semi-supervised approach: Classifying clusters with Stochastic Gradient Descent</h4>
			<p><br>Thanks to the k-means algorithm, we could visualize some clusters that were clearly identifiable with a topic extracted from the CIS. We managed to create a list of keywords affine with these topics in order to train a simple classifier. We used this classifier to label the rest of the clusters. 
			<br><br>The list consisted of a dictionary of topics from CIS and keywords from the Congreso's Corpus. This work was performed by hand, checking clusters manually as cycle of try and error. This work helped us to decide what keywords match better some concepts as: economy, employment, terrorism, housing, etc.
			<br><br>We used a Stochastic Gradient Descent (SGD) classifier to help our aim of labeling our clusters and therefore our documents. Unfortunately, we used a few documents to train the classifier because we were struggling with an unsupervised problem. However we managed to find important keywords from the most identifiable clusters to build a reliable classifier, at least to label some clusters with enough accuracy.
			<br><br>During the clustering process, we found, most of the time that it was a cluster bigger than the others. When we say “bigger” we mean, more than four sigma from the mean! So we considered splitting it up in smaller clusters and try to label them with our classifier in order to solve this problem. And this was, exactly what we finally did.</p>
			<h4><br>Building metric tables</h4>
			<p><br>After clustering every document, the final step was to craft a metric to relate documents with their importance in the Congreso. 
			<br><br>We thought deeply about this problem, and in the end, we decided that we could use the number of lines of every document as a measure of the importance of a topic, since every document was tagged to a single topic.
			<br><br>We grouped and normalized our data by year and month. We found that some months there were no data, so we applied linear interpolation to deal with these gaps. These months (without data) are: August, and those months between the change of legislature (four month approximately), we suppose politics need time to start again.</p>
			<h4><br>Miscellanea</h4>
			<p><br>During all this process, we tried some other approaches to tackle this problem, although eventually we discarded them. 
			<br><br>Some of them were:</p>
			<blockquote>
				<li>Topic model: Latent Dirichlet Analysis and Hierarchical Dirichlet Process as an alternative to k-Means algorithm</li>
				<li>TextRank algorithm as an alternative to RAKE algorithm</li>
				<li>Clustering a graph which is made up of keywords (the adjacency matrix of co-occurrence keywords). This approach gave us a small number of communities, so we decided to discard this interesting approach.</li>
			</blockquote>
			</p>
		</div>
			
		<div class="blog-post">
            <h2 class="blog-post-title">The Charts</h2>
			<p><br> At the very beginning we wanted to imitate this <a href="http://www.nytimes.com/interactive/2012/09/06/us/politics/convention-word-counts.html">chart</a> from the NY Times, but it was not enough for our purposes. It was very important for us to use the information from CIS. 
			<br><br> Having this goal in mind we created the radar and the analysis charts. Both of them with the goal of seeing how our preferences have changed over time. To make this charts we have used D3.js and we created transitions to make the visualization of the charts more dynamic. 
			<br><br> When we were close to finishing this project, we found the incredible <a href="http://politilines.periscopic.com/">PolitiLines</a> and realised that we had all the information to do it: the clusters, the keywords and the political group matched so we decided to create the Spanish version of it. 
			</p>
		</div>	

        </div><!-- /.blog-main -->

		
		<div class="col-sm-3 col-sm-offset-1 blog-sidebar">
          <div class="sidebar-module sidebar-module-inset">
            <h4>Python Packages</h4>
            <p><a href="https://selenium-python.readthedocs.org/">Selenium</a>
				<br> <a href="http://www.crummy.com/software/BeautifulSoup/bs4/doc/">BeautifulSoup</a>
				<br> <a href="http://www.nltk.org/">NLTK</a>
				<br> <a href="http://www.numpy.org/">Numpy</a>
				<br> <a href="http://pandas.pydata.org/">Pandas</a>
				<br> <a href="http://scikit-learn.org/stable/"> Scikit-Learn</a>
				<br> <a href="http://sujitpal.blogspot.in/2013/03/implementing-rake-algorithm-with-nltk.html">RAKE</a>
			</p>
			<h4>Other references:</h4>
			<p>	<li>A Python implementation of the Rapid Automatic Keyword Extraction (RAKE) algorithm as described in: Rose, S., Engel, D., Cramer, N., & Cowley, W. (2010). Automatic Keyword Extraction from Individual Documents. In M. W. Berry & J. Kogan (Eds.), Text Mining: Theory and Applications: John Wiley & Sons.</li>
				<br><li>Pattern 2.6, De Smedt, T. & Daelemans, W. (2012). Pattern for Python. Journal of Machine Learning Research, 13: 2031–2035.</li>
				<br><li>Lluís Padró— and Evgeny Stanilovsky. FreeLing 3.0: Towards Wider Multilinguality Proceedings of the Language Resources and Evaluation Conference (LREC 2012) ELRA. Istanbul, Turkey. May, 2012.</li>
				</p>
          </div>
		</div>  
    </div><!-- /.container -->

    <footer class="blog-footer">
      <p>Blog template built for <a href="http://getbootstrap.com">Bootstrap</a> by <a href="https://twitter.com/mdo">@mdo</a>.</p>
      <p>
        <a href="#">Back to top</a>
      </p>
    </footer>


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
    <script src="bootstrap/js/bootstrap.min.js"></script>
  

</body></html>