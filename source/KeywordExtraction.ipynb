{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting keywords from Congres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class to build a customize stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a list of meaningless words from a text\n",
    "from __future__ import division\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import pattern\n",
    "import nltk\n",
    "from pattern.es import tag\n",
    "\n",
    "class StopListGenerator:\n",
    "\n",
    "    def __init__(self):\n",
    "        self._dict_noun = self._get_dict_nouns()\n",
    "        self._dict_adj  = self._get_dict_adjs()\n",
    "        \n",
    "    def generate(self, text):\n",
    "        text = ' '.join(nltk.wordpunct_tokenize(text))\n",
    "        tokens_tag = tag(text.lower())\n",
    "        stoplist = []\n",
    "        for word, pos in tokens_tag:\n",
    "            if pos.startswith(\"JJ\") or pos.startswith(\"NN\"):\n",
    "                pass\n",
    "            else:\n",
    "                stoplist.append(word)\n",
    "        # Check if any from the stoplist is NN or JJ dictionary\n",
    "        l = [w for w in stoplist if w in self._dict_noun or w in self._dict_adj]\n",
    "        l = set(l)\n",
    "        \n",
    "        return list(set(stoplist) - l)\n",
    "    \n",
    "    def cleaning_text(self, text):\n",
    "        return re.sub(r'[\\xb0\\xbb\\xaa:$%#€&!¡?¿\\'()\\-\"+]+|(,/)|(\\.,)|(\\.\\.)+|(\\.\\.\\.)+|(\\d+\\.\\d*)|(\\d*\\.\\d+)|(\\d+\\.\\d+)|(\\d)',' ',text)\n",
    "            \n",
    "    def _get_dict_nouns(self):\n",
    "        p = '/Users/jccan/Dropbox/projects/SpanishPoliticsDebate/data/pickle/nam.pickle'\n",
    "        #p = 'C:\\\\Users\\\\canofran\\\\Dropbox\\\\projects\\\\SpanishPoliticsDebate\\\\data\\\\pickle\\\\nam.pickle'\n",
    "        f = open(p,'rb')\n",
    "        return pickle.load(f)\n",
    "    \n",
    "    def _get_dict_adjs(self):\n",
    "        p = '/Users/jccan/Dropbox/projects/SpanishPoliticsDebate/data/pickle/adj.pickle'\n",
    "        #p = 'C:\\\\Users\\\\canofran\\\\Dropbox\\\\projects\\\\SpanishPoliticsDebate\\\\data\\\\pickle\\\\adj.pickle'\n",
    "        f = open(p,'rb')\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def test(texto):\n",
    "    SLG = StopListGenerator()\n",
    "    stoplist = SLG.generate(texto)   \n",
    "    print stoplist\n",
    "    \n",
    "if '__name__' == '__main__':\n",
    "    test('Esto es una prueba.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class to analyze text from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "# Analyze a text to give statistics\n",
    "from __future__ import division\n",
    "\n",
    "import string\n",
    "import pattern\n",
    "import nltk\n",
    "from pattern.es import tag, tokenize\n",
    "from pattern.vector import Document\n",
    "\n",
    "def isNotPunct(word):\n",
    "    return len(word) > 1 or word not in string.punctuation\n",
    "\n",
    "class TextAnalyzer:\n",
    "    \n",
    "    def __init__(self, text):\n",
    "        self.text     = text\n",
    "        self.numwords = self._calculate_num_words()\n",
    "        self.numsents = self._calculate_num_sentences()\n",
    "        self.numlines = self._calculate_num_lines()\n",
    "    def __str__(self): \n",
    "        return str (\"Número de palabras: \" + str(self.numwords) + \"\\n\" +\n",
    "                    \"Número de sentencias: \" + str(self.numsents) + \"\\n\" +\n",
    "                    \"Número de líneas: \" + str(self.numlines))\n",
    " \n",
    "    def _calculate_num_words(self):\n",
    "        tokens = tag(' '.join(nltk.wordpunct_tokenize(self.text)))\n",
    "        words = filter(isNotPunct, [w for w, _ in tokens] )\n",
    "        return len(words)\n",
    "    \n",
    "    def _calculate_num_sentences(self):\n",
    "        return len(tokenize(self.text))\n",
    "    \n",
    "    def _calculate_num_lines(self):\n",
    "        return len(self.text.split('\\n'))\n",
    "    \n",
    "    # TODO\n",
    "    def _calculate_num_vocab(self):\n",
    "        return len(set(self.tokens))\n",
    "\n",
    "    # TODO\n",
    "    def _calculate_ratio_vocab(self):\n",
    "        return 0\n",
    "    \n",
    "    # TODO\n",
    "    def tfidf(self,top, stopwords):\n",
    "        texto = self.text\n",
    "        #patt = '\\\\b'\n",
    "        #for w in stopwords:\n",
    "        #    patt = patt+w+\"\\\\b\"+\"|\\\\b\"\n",
    "        #print patt[:-3]\n",
    "        #texto = re.sub(patt[:-3],' ', texto)\n",
    "        #print texto\n",
    "        d = Document(texto, exclude = stopwords, threshold = 1.0)\n",
    "        return d.keywords(top) # format [(weight, word),...]\n",
    "     \n",
    "                \n",
    "def test(texto):\n",
    "    AT = TextAnalyzer(texto)\n",
    "    print AT   \n",
    "\n",
    "if '__name__' == '__main__':\n",
    "    test('Esto es una prueba.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of RAKE (Rapid Automatic Keyword Extraction) algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adapted from: http://sujitpal.blogspot.com.es/2013/03/implementing-rake-algorithm-with-nltk.html\n",
    "# Adapted from: github.com/aneesha/RAKE/rake.py\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import operator\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "def isPunct(word):\n",
    "    return len(word) == 1 and word in string.punctuation\n",
    "\n",
    "def isNumeric(word):\n",
    "    try:\n",
    "        float(word) if '.' in word else int(word)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "class RakeKeywordExtractor:\n",
    "    \n",
    "    def __init__(self, stopwords=[]):\n",
    "        self.stopwords = set(stopwords)\n",
    "        self.top_fraction = 1 # consider top third candidate keywords by score\n",
    "\n",
    "    def _generate_candidate_keywords(self, sentences):\n",
    "        phrase_list = []\n",
    "        for sentence in sentences:\n",
    "            words = map(lambda x: \"|\" if x in self.stopwords else x,\n",
    "                        nltk.wordpunct_tokenize(sentence.lower()))\n",
    "            phrase = []\n",
    "            for word in words:\n",
    "                if word == \"|\" or isPunct(word):\n",
    "                    if len(phrase) > 0:\n",
    "                        phrase_list.append(phrase)\n",
    "                        phrase = []\n",
    "                else:\n",
    "                    phrase.append(word)\n",
    "        return phrase_list\n",
    "\n",
    "    def _calculate_word_scores(self, phrase_list):\n",
    "        word_freq = nltk.FreqDist()\n",
    "        word_degree = nltk.FreqDist()\n",
    "        for phrase in phrase_list:\n",
    "            degree = len(filter(lambda x: not isNumeric(x), phrase)) - 1\n",
    "            for word in phrase:\n",
    "                word_freq[word] +=1 \n",
    "                word_degree[word] += degree # other words\n",
    "        for word in word_freq.keys():\n",
    "            word_degree[word] = word_degree[word] + word_freq[word] # itself\n",
    "            # word score = deg(w) / freq(w)\n",
    "        word_scores = {}\n",
    "        for word in word_freq.keys():\n",
    "            word_scores[word] = word_degree[word] / word_freq[word]\n",
    "        return word_scores\n",
    "\n",
    "    def _calculate_phrase_scores(self, phrase_list, word_scores):\n",
    "        phrase_scores = {}\n",
    "        for phrase in phrase_list:\n",
    "            phrase_score = 0\n",
    "            for word in phrase:\n",
    "                phrase_score += word_scores[word]\n",
    "            phrase_scores[\" \".join(phrase)] = phrase_score\n",
    "        return phrase_scores\n",
    "    \n",
    "    def extract(self, text, incl_scores=False):\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        phrase_list = self._generate_candidate_keywords(sentences)\n",
    "        word_scores = self._calculate_word_scores(phrase_list)\n",
    "        phrase_scores = self._calculate_phrase_scores(\n",
    "                                               phrase_list, word_scores)\n",
    "        sorted_phrase_scores = sorted(phrase_scores.iteritems(),\n",
    "                               key=operator.itemgetter(1), reverse=True)\n",
    "        n_phrases = len(sorted_phrase_scores)\n",
    "        if incl_scores:\n",
    "            return sorted_phrase_scores[0:int(n_phrases/self.top_fraction)]\n",
    "        else:\n",
    "            return map(lambda x: x[0],\n",
    "                sorted_phrase_scores[0:int(n_phrases/self.top_fraction)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Get a static stopword list from a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List \n",
    "import pickle\n",
    "\n",
    "p = '/Users/jccan/Dropbox/projects/SpanishPoliticsDebate/data/pickle/stopwords_list.pickle'\n",
    "#p = 'C:\\Users\\canofran\\Dropbox\\projects\\SpanishPoliticsDebate\\data\\pickle\\stopwords_list.pickle'\n",
    "f = open(p,'rb')\n",
    "stopwords = pickle.load(f)\n",
    "stopwords = [unicode(w,encoding='utf-8') for w in stopwords]\n",
    "stpw = [u'señor', u'señores', u'señora', u'señoras', u'gobierno', u'presidente', u'presidenta',\n",
    "        u'gracias', u'aplausos', u'ministro',\n",
    "        u'ministra', u'ministros', u'programa', u'num_exp', u'vicepresidenta', u'partido', \n",
    "        u'diputado', u'diputada', u'euros', u'ministerio',u'usted',u'ustedes',u'señoría', u'señorías', \n",
    "        u'parlamentario', u'parlamentarios', u'grupo', u'grupos',\n",
    "        u'heredia', u'hernando',u'pérez', u'rubalcaba',u'saura', u'giménez',\n",
    "        u'socialista', u'popular',u'año',u'años', u'cámara',u'montón',u'millón', u'millones',u'número',u'números',\n",
    "        u'sector', u'sectores', u'rumores', u'tema']\n",
    "\n",
    "stopwords.extend(stpw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process of keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "# Create the connection to MongoDB\n",
    "try:\n",
    "    connection=pymongo.MongoClient()\n",
    "    print \"Connection to Mongo Daemon successful!!!\"\n",
    "except pymongo.errors.ConnectionFailure, e:\n",
    "    print \"Could not connect to MongoDB: %s\" % e\n",
    "    # Obtenim la BD del Congrés\n",
    "db = connection['congres']\n",
    "print \"Collections : \", db.collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Start Update process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a StopListGenerator object\n",
    "SLG = StopListGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# UPDATE MONGODB\n",
    "\n",
    "import datetime\n",
    "from time import time\n",
    "\n",
    "# Col.lecció de documents a la BD\n",
    "t0 = time()\n",
    "doc_col = db['document']\n",
    "\n",
    "doc_start_date = \"01/09/2000\"\n",
    "d_doc_start_date = datetime.datetime.strptime(doc_start_date, \"%d/%m/%Y\")\n",
    "doc_end_date = \"11/03/2015\"\n",
    "d_doc_end_date = datetime.datetime.strptime(doc_end_date, \"%d/%m/%Y\")\n",
    "\n",
    "result = doc_col.find({'date': {'$gte': d_doc_start_date, '$lte': d_doc_end_date}}).sort('date',1)\n",
    "\n",
    "def getkeywords(texto):\n",
    "    texto = SLG.cleaning_text(texto)\n",
    "    stoplist = SLG.generate(texto)\n",
    "    rake = RakeKeywordExtractor(stopwords = stopwords + stoplist)\n",
    "    keywords = rake.extract(texto, incl_scores=True)\n",
    "    return keywords\n",
    "\n",
    "total_keywords = []\n",
    "print \"Number of documents retrieved %d\" % result.count()\n",
    "print '=' * 40\n",
    "\n",
    "if result.count() > 0 :\n",
    "    for i, doc in enumerate(result):\n",
    "        print \"Num doc: %d\" %i \n",
    "        print \"Description: \", doc['description']\n",
    "        for dialogo in doc['session_dictionary']:\n",
    "            #print dialogo['question']\n",
    "            texto = dialogo['question']\n",
    "            keywords = getkeywords(texto) # Get its keywords\n",
    "            total_keywords.extend(keywords)\n",
    "            #print \"número de keywords encontrados: \"  + str(len(keywords))\n",
    "            kws=[]\n",
    "            for kw in keywords:\n",
    "                kws.append((kw[0],kw[1]))\n",
    "            #print kws\n",
    "            \n",
    "            dialogo[\"keywords\"] = kws\n",
    "            #print \"=\" * 20\n",
    "            #print dialogo\n",
    "            #print \"=\" * 20\n",
    "            for i, intervencion in enumerate(dialogo['intervention_dictionary']):\n",
    "                #print \"Intervenvión: \" + str(i)\n",
    "                #print \"-\" * 20\n",
    "                texto = intervencion['text']\n",
    "                keywords = getkeywords(texto)\n",
    "                total_keywords.extend(keywords)\n",
    "                #print \"número de keywords encontrados: \"  + str(len(keywords))\n",
    "                kws=[]\n",
    "                for kw in keywords:\n",
    "                    if kw[1] > 1.0:\n",
    "                        kws.append((kw[0],kw[1]))\n",
    "                #print kws\n",
    "                intervencion[\"keywords\"] = kws\n",
    "                #print intervencion\n",
    "        print '*' * 20\n",
    "        print \"Actualizo documento\"\n",
    "        print '*' * 20\n",
    "        doc_col.update({\"_id\":doc[\"_id\"]}, {\"$set\": {\"session_dictionary\": doc['session_dictionary']}})\n",
    "        \n",
    "print(\"done in %fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Update simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# No update mongodb\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Col.lecció de documents a la BD\n",
    "doc_col = db['document']\n",
    "\n",
    "t0 = time.time()\n",
    "doc_start_date = \"01/09/2000\"\n",
    "d_doc_start_date = datetime.datetime.strptime(doc_start_date, \"%d/%m/%Y\")\n",
    "doc_end_date = \"11/03/2015\"\n",
    "d_doc_end_date = datetime.datetime.strptime(doc_end_date, \"%d/%m/%Y\")\n",
    "\n",
    "result = doc_col.find({'date': {'$gte': d_doc_start_date, '$lte': d_doc_end_date}}).sort('date',1)\n",
    "\n",
    "def getkeywords(texto):\n",
    "    texto = SLG.cleaning_text(texto)\n",
    "    stoplist = SLG.generate(texto)\n",
    "    rake = RakeKeywordExtractor(stopwords = stopwords + stoplist)\n",
    "    keywords = rake.extract(texto, incl_scores=True)\n",
    "    return keywords\n",
    "\n",
    "#keywords = []\n",
    "count_doc = []\n",
    "if result.count() > 0 :\n",
    "    print \"Documentos a tratar: %d\" % result.count()\n",
    "    for doc in result:\n",
    "        print doc['description']\n",
    "        for dialogo in doc['session_dictionary']:\n",
    "            keywords2=[]\n",
    "            #print dialogo.keys()\n",
    "            #print dialogo['question']\n",
    "            texto = dialogo['question']\n",
    "            #keywords.extend(getkeywords(texto))\n",
    "            keywords2.extend(getkeywords(texto))\n",
    "            #print keywords\n",
    "            for idx, intervencion in enumerate(dialogo['intervention_dictionary']):\n",
    "                #print \"Intervenvión: \" + str(idx)\n",
    "                #print \"-\" * 20\n",
    "                #print intervencion.keys()\n",
    "                #print intervencion[\"keywords\"]\n",
    "                texto = intervencion[\"text\"]\n",
    "                #keywords.extend(getkeywords(texto))\n",
    "                keywords2.extend(getkeywords(texto))\n",
    "                #print keywords\n",
    "            count_doc.append([idx, keywords2])\n",
    "        #print \"=\"*80\n",
    "        \n",
    "#df = pd.DataFrame(keywords)\n",
    "df2 = pd.DataFrame(count_doc, columns=[\"NumInterventions\", \"ListKws\"])\n",
    "# All keywords\n",
    "df2.to_pickle('./files/kw_per_doc.pkl')\n",
    "\n",
    "#df.to_csv('kws_sim.csv', encoding='utf-8')\n",
    "print \"*** Fin del proceso ***\"\n",
    "print(\"done in %fs\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = df2['kw'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kw = l[0][0]\n",
    "score = l[0][1]\n",
    "print kw ,\"->\", score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check strange characters in the extracted keywords\n",
    "print df[df[0].str.contains(r'\\b[a-zA-Z\\xe1\\xe9\\xed\\xf3\\xfa\\xfc\\xc1\\xc9\\xcd\\xd3\\xda\\xdc\\xf1]+\\b') == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(df[df[1] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_pickle('./files/allkeywords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allkw = pd.read_pickle('./files/allkeywords.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(allkw[allkw[1]>4.0][0].values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check keywords from documents\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk import wordpunct_tokenize\n",
    "\n",
    "\n",
    "# Col.lecció de documents a la BD\n",
    "doc_col = db['document']\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "doc_start_date = \"01/09/2000\"\n",
    "d_doc_start_date = datetime.datetime.strptime(doc_start_date, \"%d/%m/%Y\")\n",
    "doc_end_date = \"11/03/2015\"\n",
    "d_doc_end_date = datetime.datetime.strptime(doc_end_date, \"%d/%m/%Y\")\n",
    "\n",
    "result = doc_col.find({'date': {'$gte': d_doc_start_date, '$lte': d_doc_end_date}}).sort('date',1)\n",
    "\n",
    "keyws=[]\n",
    "num_sesiones = 0\n",
    "ndoc_general = 0\n",
    "if result.count() > 0 :\n",
    "    for doc in result:\n",
    "        #print doc['description']\n",
    "        ndoc_interno = 0\n",
    "        for dialogo in doc['session_dictionary']:\n",
    "            for ele in dialogo['keywords']:\n",
    "                keyws.append((ele[0], ele[1]))                                                \n",
    "            for intervencion in dialogo['intervention_dictionary']:\n",
    "                for ele in intervencion['keywords']:\n",
    "                    keyws.append((ele[0], ele[1])) \n",
    "\n",
    "df = pd.DataFrame(keyws)\n",
    "df.to_csv('kws.csv', encoding='utf-8')\n",
    "\n",
    "print \"Fin del proceso\"\n",
    "print(\"done in %fs\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of keywords loaded in mongodb\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check keywords from documents\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk import wordpunct_tokenize\n",
    "\n",
    "\n",
    "# Col.lecció de documents a la BD\n",
    "doc_col = db['document']\n",
    "\n",
    "doc_start_date = \"01/09/2000\"\n",
    "d_doc_start_date = datetime.datetime.strptime(doc_start_date, \"%d/%m/%Y\")\n",
    "doc_end_date = \"11/03/2015\"\n",
    "d_doc_end_date = datetime.datetime.strptime(doc_end_date, \"%d/%m/%Y\")\n",
    "\n",
    "result = doc_col.find({'date': {'$gte': d_doc_start_date, '$lte': d_doc_end_date}}).sort('date',1)\n",
    "\n",
    "keywords=[]\n",
    "num_sesiones = 0\n",
    "ndoc_general = 0\n",
    "if result.count() > 0 :\n",
    "    for doc in result:\n",
    "        print doc['description']\n",
    "        ndoc_interno = 0\n",
    "        for dialogo in doc['session_dictionary']:\n",
    "            texto = dialogo['question']\n",
    "            nlines = 0\n",
    "            tokens = wordpunct_tokenize(texto) \n",
    "            #          if token not in re.findall(r'[!\"#$%&\\\\\\'()*+\\,\\-\\./:;<=>?@[\\\\\\]^_`{|}~\\xbf\\xbf\\xa1]+',texto)]\n",
    "            #pprint (tokens)\n",
    "            keywords.append((num_sesiones, ndoc_general, ndoc_interno, 0, len(dialogo['keywords']), \n",
    "                             len(tokens), nlines, doc['date']))\n",
    "            ndoc_interno +=1\n",
    "            for intervencion in dialogo['intervention_dictionary']:\n",
    "                texto = intervencion[\"text\"]\n",
    "                nlines = 0\n",
    "                tokens = wordpunct_tokenize(texto) \n",
    "                #      if token not in re.findall(r'[!\"#$%&\\\\\\'()*+\\,\\-\\./:;<=>?@[\\\\\\]^_`{|}~\\xbf\\xbf\\xa1]+',texto)]\n",
    "                #pprint (tokens)\n",
    "                keywords.append((num_sesiones, ndoc_general, ndoc_interno, 1, len(intervencion['keywords']), \n",
    "                                 len(tokens), nlines ,doc['date']))\n",
    "                ndoc_interno +=1\n",
    "            ndoc_general +=1\n",
    "        num_sesiones +=1\n",
    "\n",
    "df = pd.DataFrame(keywords,columns=['ndiario', 'ndoc_g', 'ndoc_i', 'tipo','nkws','nwords', 'nlines', 'data'])\n",
    "\n",
    "# uncomment if you want to save as a cvs file\n",
    "# df.to_csv('docs_stat.csv')\n",
    "\n",
    "print \"Fin del proceso\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
